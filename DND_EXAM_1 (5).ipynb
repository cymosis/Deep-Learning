{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Exam for Deep Network Development course**\n",
        "Name:\n",
        "\n",
        "Neptun ID:\n",
        "\n",
        "Date: 27/05/2024\n",
        "\n",
        "Duration: 9AM-11AM"
      ],
      "metadata": {
        "id": "hY88PZuGRlvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General rules\n",
        "This notebook contains the task to be solved in order to pass the exam and complete the course.\n",
        "It contains a task similar to what you have worked on during the semester, which consists on implementing a network architecture and a function. There are additional requirements which are optional.\n",
        "\n",
        "The exam has a duration of 2 hours. You are free to distribute the time as you please between the different requirements.\n",
        "During the exam you can use any resource (internet, AI, practice notebooks, etc). **However it is strictly prohibited to use any communication channel** (Teams, WhatsApp, Messenger, etc.). Using any of those will result in immediate **FAIL**.\n",
        "\n",
        "Your solution should be submitted to Canvas as a .ipynb file!\n",
        "\n",
        "Please note that, to **PASS** the exam you must **SUBMIT A SUCCESSFUL SOLUTION SATISFYING THE MINIMUM REQUIREMENTS**. If you **FAIL** the exam, you have the right to retry it **ONE MORE TIME**. If you **FAIL AGAIN**, then unfortunately, you have failed the course.\n",
        "\n",
        "If you **PASS** the exam, then the final grade is the weighted average of your asignment defenses (theory and code)."
      ],
      "metadata": {
        "id": "994g3vW4RpXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task description & Requirements\n",
        "The task is to implement a custom architecture and its forward function.\n",
        "The task is inspired on Image Captioning and the architecture is a U-Net like model with an LSTM at the end. The model receives an image as input and generates text as output (actually a probability distribution over a set of tokens). For the extra part, you are required to combine different inputs and to extend the architecture."
      ],
      "metadata": {
        "id": "b4tV5mZSksUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements:\n",
        "------------------------------------------------------------------------\n",
        "**Minimum requirements - ENOUGH TO PASS THE EXAM**\n",
        "1.   Implement the layers of the architecture shown in section 1. Fill out the unknown parts in order to complete the architecture.\n",
        "2.   Implement the forward function of the architecture. Make sure that the input and output are correct.\n",
        "\n",
        "**!!! To complete the requirements 1 and 2, your final output should match the expected output indicated on cell 1.2. !!!**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Extra requirements - for grade improvement and potentially access to AI Lab**\n",
        "3.   Use the output embedding of the encoder implemented on cell 1.1 and fuse it with another provided embedding. --> **+1 in final grade**\n",
        "4.   Modify the architecture previously implemented to accept the new fused embedding and extend the architecture with the details shown on cell 1.4. --> **Access to AI Lab**\n",
        "------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "fgqsWZ2wus0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "17TpFFozzSub"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUlHmH168QUL",
        "outputId": "576fa6d6-9f68-45a1-ffe0-04d43c2248b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Architecture\n",
        "\n",
        "Please right click the image and \"Open image in a new tab\" to view it better with zoom. Or download it from here: https://drive.google.com/file/d/1P3q6dNxywzIpkmOhxhqN01I5NVlQWW7_/view?usp=sharing\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1P3q6dNxywzIpkmOhxhqN01I5NVlQWW7_)"
      ],
      "metadata": {
        "id": "ZpUl7bMKwXjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stride_pad(input_size,output_size,kernel_size,padding):\n",
        "  output_size=output_size-1\n",
        "  stride=(input_size-kernel_size+(2*padding))/(output_size)\n",
        "  return stride\n",
        "\n",
        "get_stride_pad(127,127,3,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO08jL8HHCCz",
        "outputId": "626c6c30-51d0-4a72-ebf1-de6fb411f1bd"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Implement the architecture and its forward function"
      ],
      "metadata": {
        "id": "3aT0ruQfwbJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomNet, self).__init__()\n",
        "        self.upper_1=nn.Conv2d(3,16,kernel_size=3,stride=1,padding=0)\n",
        "        self.upper_11=nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.upper_2=nn.Conv2d(3,16,kernel_size=3,stride=1,padding=0)\n",
        "        self.upper_22=nn.Conv2d(16,16,kernel_size=3,stride=2,padding=1)\n",
        "\n",
        "        self.layer_3=nn.Sequential(nn.Conv2d(16,32,kernel_size=3,stride=1,padding='same'),\n",
        "                                   nn.ReLU())\n",
        "        self.layer_4=nn.Conv2d(16,32,kernel_size=3,stride=1,padding='same')\n",
        "        self.layer_44=nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1)\n",
        "\n",
        "        #Define layers\n",
        "        self.layer_5=nn.Sequential(nn.Conv2d(64,128,kernel_size=5,stride=2,padding=0),\n",
        "                                   nn.BatchNorm2d(128))\n",
        "        #bottleneck\n",
        "        self.layer_6=nn.Sequential(nn.ConvTranspose2d(128,64,kernel_size=3,stride=1,padding=0),\n",
        "                                   nn.ReLU())\n",
        "        self.layer_66=nn.ConvTranspose2d(64,64,kernel_size=3,stride=2,padding=1)\n",
        "        #upsampling\n",
        "        self.decoder=nn.Sequential(nn.ConvTranspose2d(64,3,kernel_size=4,stride=2,padding=0),\n",
        "                                   nn.Conv2d(3,16,kernel_size=11,stride=8,padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input:\", x.shape)\n",
        "        print()\n",
        "        x1=self.upper_1(x)\n",
        "        print(\"Upper_1:\", x1.shape)\n",
        "        print()\n",
        "        x1=self.upper_11(x1)\n",
        "        print(\"Upper_11:\", x1.shape)\n",
        "        print()\n",
        "        x2=self.upper_2(x)\n",
        "        print(\"Upper_2:\", x2.shape)\n",
        "        print()\n",
        "        x2=self.upper_22(x2)\n",
        "        print(\"Upper_22:\", x2.shape)\n",
        "        print()\n",
        "        x3=x1+x2\n",
        "        print(\"Concat:\", x3.shape)\n",
        "        print()\n",
        "        x4=self.layer_3(x3)\n",
        "        print(\"Layer_3:\", x4.shape)\n",
        "        print()\n",
        "        x5=self.layer_4(x3)\n",
        "        print(\"Layer_4:\", x5.shape)\n",
        "        print()\n",
        "        x5=self.layer_44(x5)\n",
        "        print(\"Layer_44:\", x5.shape)\n",
        "        print()\n",
        "        x6=torch.cat([x4,x5], dim=1)\n",
        "        print(\"Concat2:\", x6.shape)\n",
        "        print()\n",
        "        x7=self.layer_5(x6)\n",
        "        print(\"Layer_5:\", x7.shape)\n",
        "        print()\n",
        "        x8=self.layer_6(x7)\n",
        "        print(\"Layer_6:\", x8.shape)\n",
        "        print()\n",
        "        x8=self.layer_66(x8)\n",
        "        print(\"Layer_66:\", x8.shape)\n",
        "        print()\n",
        "\n",
        "\n",
        "        x9=x8+x6\n",
        "        print(\"Concat:\", x9.shape)\n",
        "        print()\n",
        "        x10=self.decoder(x9)\n",
        "        print(\"Decoder:\", x10.shape)\n",
        "        print()\n",
        "        x11 = x10.permute(0, 2, 3, 1)     # [B, H, W, C] = [1, 15, 15, 16]\n",
        "        x11 = x11.reshape(x10.size(0), -1, x10.size(1))  # [1, 225, 16]\n",
        "        print(\"Reshape:\", x11.shape)\n",
        "        print()\n",
        "        lstm=nn.LSTM(16,hidden_size=64,num_layers=1,batch_first=True)\n",
        "        x12,_=lstm(x11)\n",
        "        print(\"LSTM:\", x12.shape)\n",
        "        print()\n",
        "        softmax=nn.Softmax2d()\n",
        "        x13=softmax(x12)\n",
        "        print(\"LSTM:\", x13)\n",
        "        out=x13\n",
        "        # Define the encoder part\n",
        "        print(\"Encoder\")\n",
        "        print()\n",
        "\n",
        "        #Define the decoder part\n",
        "        print(\"Decoder\")\n",
        "        print()\n",
        "\n",
        "        # PLEASE NAME YOUR FINAL OUTPUT AS 'out' OR RENAME THE VARIABLE IN THE RETURN\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "bfu97zIywjSm"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Check if your implementation is correct\n",
        "For a given arbitraty input of size (1,3,256,256) the expected output is (1,961,64)"
      ],
      "metadata": {
        "id": "7b6sZHXGwkcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.tensor(np.random.rand(3,256,256), dtype=torch.float32)\n",
        "image = torch.unsqueeze(image, dim=0)\n",
        "\n",
        "model = CustomNet()\n",
        "output = model(image)\n",
        "print()\n",
        "\n",
        "try:\n",
        "    assert output.shape == torch.Size([1, 961, 64])\n",
        "    print(\"CONGRATULATIONS! You have PASSed the exam by successfully completing the minimum requirements!\")\n",
        "except AssertionError:\n",
        "    print(\"Unfortunately, you have FAILed the exam by not being able to complete the minimum requirements.\")"
      ],
      "metadata": {
        "id": "unXn-RdUw5l5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3253561-644c-495f-bb70-b2a3ea5e8dc2"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: torch.Size([1, 3, 256, 256])\n",
            "\n",
            "Upper_1: torch.Size([1, 16, 254, 254])\n",
            "\n",
            "Upper_11: torch.Size([1, 16, 127, 127])\n",
            "\n",
            "Upper_2: torch.Size([1, 16, 254, 254])\n",
            "\n",
            "Upper_22: torch.Size([1, 16, 127, 127])\n",
            "\n",
            "Concat: torch.Size([1, 16, 127, 127])\n",
            "\n",
            "Layer_3: torch.Size([1, 32, 127, 127])\n",
            "\n",
            "Layer_4: torch.Size([1, 32, 127, 127])\n",
            "\n",
            "Layer_44: torch.Size([1, 32, 127, 127])\n",
            "\n",
            "Concat2: torch.Size([1, 64, 127, 127])\n",
            "\n",
            "Layer_5: torch.Size([1, 128, 62, 62])\n",
            "\n",
            "Layer_6: torch.Size([1, 64, 64, 64])\n",
            "\n",
            "Layer_66: torch.Size([1, 64, 127, 127])\n",
            "\n",
            "Concat: torch.Size([1, 64, 127, 127])\n",
            "\n",
            "Decoder: torch.Size([1, 16, 31, 31])\n",
            "\n",
            "Reshape: torch.Size([1, 961, 16])\n",
            "\n",
            "LSTM: torch.Size([1, 961, 64])\n",
            "\n",
            "LSTM: tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<SoftmaxBackward0>)\n",
            "Encoder\n",
            "\n",
            "Decoder\n",
            "\n",
            "\n",
            "CONGRATULATIONS! You have PASSed the exam by successfully completing the minimum requirements!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "yVZ_v02k7v_G"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTRA REQUIREMENTS (OPTIONAL)"
      ],
      "metadata": {
        "id": "6obCCSF6ydNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Fuse the embeddings\n",
        "First obtain the image embeddings from the last layer of the encoder from the previously implemented architecture. Then combine it with the new provided embedding.\n",
        "\n",
        "Please right click the image and \"Open image in a new tab\" to view it better with zoom. Or download it from here: https://drive.google.com/file/d/1pts-Dzka5fYD6clW3Pb1NCgEbPWZKF74/view?usp=sharing\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1pts-Dzka5fYD6clW3Pb1NCgEbPWZKF74)"
      ],
      "metadata": {
        "id": "RGnGijcPyPG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombineEmbeddings(nn.Module):\n",
        "    def __init__(self, embed_dim, comb_dim):\n",
        "        super(CombineEmbeddings, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.comb_dim = comb_dim\n",
        "\n",
        "    def forward(self, embedding_1, embedding_2):\n",
        "        #Reshaping might be needed\n",
        "\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "AUnOBmL3P_UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REPLACE THIS WITH ACTUAL EMBEDDING FROM ENCODER OUTPUT OF PREVIOUSLY IMPLEMENTED ARCHITECTURE.\n",
        "#FOR SIMPLICITY REMOVE BATCH SIZE - squeeze\n",
        "encoder_embedding = torch.randn(128, 62, 62) #REPLACE THIS!!!!\n",
        "\n",
        "#NEW EMBEDDING\n",
        "new_embedding = torch.randn(128, 62, 62)\n",
        "\n",
        "#combine embeddings according to the figure in 1.3.\n",
        "combine = CombineEmbeddings(embed_dim=128, comb_dim=128) #for simplicity use the same size for the combination 128\n",
        "output = combine(encoder_embedding, new_embedding)\n",
        "print()\n",
        "\n",
        "try:\n",
        "    assert output.shape == torch.Size([128, 62, 62])\n",
        "    print(\"CONGRATULATIONS! You have earned +1 in your final grade by successfully satisfying one of the extra requirements!\")\n",
        "except AssertionError:\n",
        "    print(\"Sorry! Keep trying!\")"
      ],
      "metadata": {
        "id": "RKcQGExVyR62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. Modify and extend the architecture\n",
        "Remove all the existing decoder layers and use just a Transformer based decoder. The input should be the new combined embedding."
      ],
      "metadata": {
        "id": "GBPtTVDHyTDE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v89rb89FyWck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"If you have completed this part, then CONGRATULATIONS! You will be suggested as a potential student to join the AI Lab!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D50LNAPtRL7O",
        "outputId": "c185dbe4-46a0-4f73-ba4c-eebf125c3e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you have completed this part, then CONGRATULATIONS! You will be suggested as a potential student to join the AI Lab!\n"
          ]
        }
      ]
    }
  ]
}